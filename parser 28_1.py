'''Методические указания'''
'''
Урок 28.1 Углубленный анализ кода
Задачи урока:
Углубленный анализ кода

1. Углубленный анализ кода

Учитель: До сих пор нам встречались два типа объектов библиотеки BeautifulSoup:
•объектыBeautifulSoup — экземпляры, которые в предыдущих примерах кода встречались в виде переменной bs;
• объектыTag — в виде списков или отдельных элементов, как результаты вызовов функций find и find_all для объекта BeautifulSoup или полученные при проходе по структуре объекта BeautifulSoup:
bs.div.h1

Однако в библиотеке есть еще два объекта, которые используются реже, но все же о них важно знать:
•объектыNavigableString — служат для представления не самих тегов, а текста внутри тегов (некоторые функции принимают и создают не объекты тегов, а объекты NavigableString);
•объектыComment — применяются для поиска HTML-комментариев, заключенных в теги комментариев, <!--например,так-->.

Из всей библиотеки BeautifulSoup вам в большинстве случаев придется иметь дело только с этими четырьмя объектами.

Функция find_all выполняет поиск тегов по их именам и атрибутам. Но как быть, если нужно найти тег по его расположению в документе? Здесь нам пригодится навигация по дереву. Ранее мы встречали навигацию по дереву BeautifulSoup только в одном направлении:
bs.tag.subTag.anotherSubTag

Теперь рассмотрим навигацию по деревьям HTML-кода во всех направлениях: вверх, по горизонтали и диагонали. В качестве образца для веб-скрапинга мы будем использовать
тестовый интернет-магазин, размещенный по адресу http://www.pythonscraping.com/pages/page3.html.

HTML-код этой страницы, представленный в виде дерева (некоторые теги для краткости опущены), выглядит так:
HTML
— body
    — div.wrapper
         — h1
         — div.content
         — table#giftList
             — tr
                   — th
                   — th
                   — th
                   — th
          — tr.gift#gift1
                   — td
                   — td
                         — span.excitingNote
                  — td
                  — td
                        — img
          — ...другие строки таблицы...
   — div.footer

В BeautifulSoup, как и во многих других библиотеках, существует различие между детьми и потомками:
 как и в генеалогическом древе любого человека, дети всегда располагаются ровно на один уровень ниже родителей, 
 тогда как потомки могут находиться на любом уровне дерева ниже родителя. Скажем, теги tr являются детьми тега table, а теги
tr, th, td, img и span — потомками тега table (по крайней мере, в нашем примере). Все дети — потомки, но не все потомки — дети.

В целом функции BeautifulSoup всегда имеют дело с потомками тега, выбранного в данный момент. Например, функция bs.body.h1 выбирает первый тег h1,
 который является потомком тега body. Она не найдет теги, расположенные за пределами body.
Аналогично функция bs.div.find_all('img') найдет первый тег div в документе, а затем извлечет список всех тегов img, которые являются потомками этого тега div.
Получить только тех потомков, которые являются детьми, можно с помощью тега .children:
'''
def a1():
   from urllib.request import urlopen
   from bs4 import BeautifulSoup
   html = urlopen('https://www.pythonscraping.com/pages/page3.html')
   bs = BeautifulSoup(html, 'html.parser')
   for child in bs.find('table',{'id':'giftList'}).children: # берем тольео потомков !!!!children!!!! (bs.find)- библиотека
      print(child)


'''
Данный код выводит список всех строк таблицы giftList, в том числе начальную строку с заголовками столбцов. Если вместо функции children() в этом коде использовать
функцию desndants(), то она найдет в таблице и выведет примерно два десятка тегов, включая img, span и отдельные теги td.

Функция next_siblings() библиотеки BeautifulSoup упрощает сбор данных из таблиц, особенно если в таблице есть заголовки:
'''
def a2():
   from urllib.request import urlopen
   from bs4 import BeautifulSoup
   html = urlopen('https://www.pythonscraping.com/pages/page3.html')
   bs = BeautifulSoup(html, 'html.parser')
   for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings: # jбращаемся к братьям
      print(sibling)



'''
Этот код должен выводить все строки таблицы, кроме первой с заголовком. Почему пропускается строка заголовка? 
Потому что объект не может быть сиблингом сам себе. Каждый раз, когда составляется список сиблингов (братьев и сестер) объекта,
 сам объект не включается в этот список. Как следует из названия, данная функция выбирает только следующих по списку сиблингов.
  Например, если выбрать строку, расположенную в середине таблицы, и вызвать для нее функцию next_siblings, то функция вернет только тех
сиблингов, которые идут в списке после данной строки. Таким образом, выбрав строку заголовка и вызвав функцию next_siblings, мы получим все строки таблицы, кроме
самой строки заголовка.
У функции next_siblings есть парная функция previous_siblings. Она часто бывает полезна, если в конце списка одноуровневых тегов, который вы хотели бы получить,
 есть легко выбираемый тег.
И конечно же, существуют функции next_sibling и previous_sibling, которые выполняют почти то же, что и next_siblings и previous_siblings,
 но только возвращают не список тегов, а лишь один тег.

При сборе данных со страниц вы, скорее всего, быстро поймете, что выбирать родительский тег необходимо реже, чем детей или сиблингов.
 Как правило, просмотр HTML-страницы с
целью поиска данных мы начинаем с тегов верхнего уровня, после чего ищем способ углубиться в нужный фрагмент данных.
 Однако иногда встречаются странные ситуации, когда приходится использовать функции поиска родительских элементов .parent и .parents из библиотеки BeautifulSoup.
'''
def a3():
   from urllib.request import urlopen
   from bs4 import BeautifulSoup

   html = urlopen('https://www.pythonscraping.com/pages/page3.html')
   bs = BeautifulSoup(html, 'html.parser')
   print(bs.find('img', {'src': '../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())



'''
До сих пор мы исследовали способы доступа к тегам и их контенту и способы их фильтрации. Однако часто при парсине нас интересует не содержимое тега, а его атрибуты.
Это особенно полезно для таких тегов, как a, в атрибуте href содержащих URL, на которые ссылаются эти теги, или же тегов img, в атрибуте src содержащих ссылки на целевые
изображения.

Python позволяет автоматически получить список атрибутов для объекта тега, вызвав следующую функцию:
myTag.attrs
Эта функция возвращает в чистом виде словарь Python, благодаря чему получение атрибутов и управление ими становится тривиальной задачей. Например, для того, чтобы
узнать, где находится файл с изображением, можно воспользоваться следующим кодом:
myImgTag.attrs['src']

В Beautiful Soup можно применять также и регулярные выражения. На них мы особо заострять внимание не будем,
 но кому интересно обязательно обратите внимание на модуль re стандартной библиотеки Python.
'''
def a4():

   from urllib.request import urlopen
   from bs4 import BeautifulSoup
   import re
   html = urlopen('https://www.pythonscraping.com/pages/page3.html')
   bs = BeautifulSoup(html, 'html.parser')
   images = bs.find_all('img', {'src':re.compile('..\/img\/gifts/img.*.jpg')}) #\/-пкть
   for image in images:
      print(image['src'])



'''
К сожалению, изучение регулярных выражений (часто сокращаемых до regex) часто сводится к пространным таблицам случайных символов, сочетания которых выглядят
как абракадабра. Это отпугивает многих людей, а потом они решают рабочие задачи и пишут ненужные сложные функции поиска и фильтрации,
 хотя можно было бы обойтись всего одной строкой с регулярным выражением!
К счастью, регулярные выражения не так уж трудно быстро освоить. Чтобы их изучить, достаточно рассмотреть всего несколько простых примеров и поэкспериментировать с
ними.

Регулярные выражения получили свое название благодаря тому, что используются для идентификации регулярных строк.
 Регулярное выражение позволяет сделать однозначный вывод о том, что данная строка соответствует определенным правилам, и вернуть ее или же сделать вывод о том,
  что строка не соответствует правилам, и отбросить ее. Это невероятно
удобно для быстрой проверки больших документов, в которых нужно найти номера телефонов или адреса электронной почты.
Мы использовали выражение «регулярная строка». Что это такое? Это любая строка, которую можно построить с учетом последовательности линейных правил 3 следующего вида.
1. Написать хотя бы одну букву a.
2. Добавить ровно пять букв b.
3. Добавить произвольное четное число букв c.
4. В конце поставить букву d или e.
Этим правилам соответствуют строки
 aaaabbbbbccccd,
aabbbbbcce и т.д. (количество вариантов бесконечно).

Регулярные выражения — всего лишь краткий способ представления этих наборов правил. Например, регулярное выражение для описанного выше набора правил выглядит так:
aa*bbbbb(cc)*(d|e)
На первый взгляд эта строка смотрится жутковато, но она станет понятнее, если разбить ее на составляющие:
•aa* — буква a, после которой стоит символ * (звездочка), означает «любое количество букв a, включая 0». Такая запись гарантирует, что буква a будет написана хотя бы
один раз;
• bbbbb — ничего особенного, просто пять букв b подряд;
• (cc)* — любое количество чего угодно можно заключить в скобки. Поэтому для реализации правила о четном количестве букв c мы можем написать две буквы c,
 заключить их в скобки и поставить после них звездочку. Это значит, что в строке может присутствовать любое
количество пар, состоящих из букв c (обратите внимание, что это также может означать 0 пар);
• (d|e) — вертикальная линия между двумя выражениями означает «то или это». В данном случае мы говорим «добавить d или e».
 Таким образом мы гарантируем, что в строку добавится ровно один из этих двух символов.


1
^ — соответствует началу строки.
2
$— соответствует концу строки.
3
. — соответствует любому символу, кроме новой строки. Использование флага re.M позволяет также соответствовать новой строке.
4
[4fw] — соответствует любому из символов в скобках.
5
[^4fw] — соответствует любому символу, кроме тех, что в квадратных скобках.
6
foo* — соответствует 0 или более вхождений “foo”.
7
bar+ —- соответствует 1 или более вхождениям “bar”.
8
foo? —- соответствует 0 или 1 вхождению “foo”.
9
bar{3} —- соответствует трем подряд вхождениям “bar”.
10
foo{3,} — соответствует 3 или более вхождениям “foo”.
11
bar{2,5} —- соответствует от 2 до 5 вхождениям “bar”.
12
a|b — соответствует либо a, либо b.
13
(foo) — группирует регулярные выражения.
14
(?imx) — временно включает параметры i, m или x в регулярное выражение. Если используются круглые скобки — затрагивается только эта область.
15
(?-imx) — временно отключает опции i, m или x в регулярном выражении. Если используются круглые скобки — затрагивается только эта область.
16
(?: foo) — Группирует регулярные выражения без сохранения совпадающего текста.
17
(?imx: re) — Временно включает параметры i, m или x в круглых скобках.
18
(?-imx: re) — временно отключает опции i, m или x в круглых скобках.
19
(?#…) — комментарий.
20
(?= foo) — совпадает со всеми словами после которых » foo».
21
(?! foo) — совпадает со всеми словами после которых нет » foo».
22
(?> foo) — совпадает со всеми словами перед которыми » foo».
23
\w — совпадает с буквенным символом.
24
\W — совпадает с не буквенным символом.
25
\s — совпадает с пробельными символами (\t, \n, \r, \f и пробелом).
26
\S — все кроме пробельных символов.
27
\d — соответствует цифрам (0-9).
28
\D — все кроме цифры.
29
\A — соответствует началу строки.
30
\Z – соответствует концу строки. Включая перевод на новую строку, если такая есть.
31
\z — соответствует концу строки.
32
\G — соответствует месту, где закончилось последнее соответствие.
33
\b — соответствует границам слов, когда поставлены внешние скобки.
34
\B — все кроме границы слова.
35
**\n,\t,\r,\f ** — соответствует новым строкам, подстрокам.
36
\1…\9 — соответствует подгруппе n-й группы.
37
\10 — соответсвуйет номеру группы. В противном случае относится к восьмеричному представлению символьного кода.


В отношении парсинга BeautifulSoup и регулярные выражения идут рука об руку.
 В сущности, функция, принимающая строку в качестве аргумента (например, find(id="идентификаторТега")), скорее всего, будет принимать и регулярное выражение.

Рассмотрим несколько примеров, проверив страницу по адресу http://www.python scraping.com/pages/page3.html. На этом сайте есть много изображений
товаров, представленных в таком виде:
<img src="../img/gifts/img3.jpg">

Если мы хотим собрать URL всех изображений товаров, то на первый взгляд решение может показаться довольно простым:
 достаточно выбрать все теги изображений с помощью функции .find_all("img"), верно? Не совсем.

Кроме очевидных «лишних» изображений (например, логотипов), на современных сайтах часто встречаются скрытые и пустые изображения, используемые вместо пробелов и для
выравнивания элементов, а также другие случайные теги изображений, о которых вы, возможно, не знаете. Определенно нельзя рассчитывать на то,
 что все изображения на странице являются только изображениями товаров.
Предположим также, что макет страницы может изменяться или по какой-либо причине поиск правильных тегов не должен зависеть от расположения изображений на странице.
 Например, вы хотите собрать определенные элементы или фрагменты данных, разбросанные по всему сайту случайным образом.
  Так, для титульного изображения товара может быть предусмотрено специальное место наверху некоторых — но не всех — страниц.

Решение состоит в поиске чего-то идентифицирующего сам тег. В данном случае можно поискать путь к файлам изображений товаров
'''

def a6():
   from urllib.request import urlopen
   from bs4 import BeautifulSoup
   import re
   html = urlopen('http://www.pythonscraping.com/pages/page3.html')
   bs = BeautifulSoup(html, 'html.parser')
   images = bs.find_all('img', {'src':re.compile('..\/img\/gifts/img.*.jpg')})
   for image in images:
      print(image['src'])



'''
Этот код выводит только те относительные пути к изображениям, которые начинаются с ../img/gifts/img и заканчиваются на .jpg
Регулярное выражение может использоваться как аргумент в выражении, написанном на BeautifulSoup, что обеспечивает большую гибкость при поиске нужных элементов.

 Давайте сегодня разберем еще один вариант поиска элементов на странице, при помощи XPath.

Когда мы не можем найти уникальный селектор для необходимого нам элемента на помощь приходит поиск по XPath, поиск атрибута по его пути.

XPath - очень мощный и гибкий инструмент, но в тот же момент достаточно нестабильный.
XPath помогает писать сложные запросы поиска элементов
Если разработчик сайта как то изменить структуру сайта, то работа парсера будет некорректной, так как искомый элемент уже не будет найден
XPath использует древовидную структуру документа. Проверять XPath запросы можно точно так же, как и обычные css селекторы - в консоли разработчика.
Стоит помнить, что XPath всегда начинается с символа / или //
Пример:
element1/element2 - выбирает элементы element2, являющиеся прямыми потомками element 1
element1/element2 - выбирает элементы element2, являющиеся потомками element 1 любой степени вложенности

Разница состоит в том что в когда мы начинаем запрос с символа /, мы должны указать элемент являющийся корнем нашего документа.
 Корнем всегда будет элемент с тегом <html>
Пример: /html/body/header

Мы можем начинать запрос и символа //. Это будет означать, что мы хотим найти всех потомков корневого элемента без указания корневого элемента.

Давайте немного попрактикуемся. Откроем страничку и откроем консоль разработчика с помощью клавиши F12.


Теперь попробуе найти заголовок страницы с помощью XPath.
 Для начала нажмем на структуру нашей страницы в консоли разработчика и нажмем комбинацию клавиш ctrl + f для открытия строки поиска.
  В данном случае, так как заголовок у нас всего один, то мы можем ввести в строке поиска //h1, что означает искать тег h1 на любом уровне вложенности


То же самое можно проделать, например с таблицей. Введем запрос //table//tr/td и мы увидим, что выделился первый тег td, в теге tr,
 который в свою очередь находится в теге table, а в самом поиске мы увидим, что результатов поиска порядка 20 штук.


Думаю с этой механикой мы разобрались. Пойдем дальше
Символ [], является командой фильтрации. Если по запросу найдено несколько элементов, то будет произведена фильтрация по правилу, указанному в скобках.

Разберем основные правила фильтрации
по любому атрибуту (id, class, title и т.п). Например если мы хотим найти первую строку в нашей таблице, мы можем увидеть, что она имеет id gift1.
 Значит, что мы можем сделать запрос //tr[@id='gift1']
по порядковому номеру. Например, мы хотим выбрать третью строку в таблице с классом gift, тогда запрос будет //tr[@class='gift'][3]
по полному совпадению текста. Например мы хотим найти элемент, где указан заголовок, то можем сформировать запрос //*[text()='Totally Normal Gifts'].
 Такой селектор вернет элемент, только если текст полностью совпадает. Отметим, что поиск по тексту не самая лучшая практика.
по частичному совпадению текста или атрибута. Для данного поиска используется функция contains. Для поиска например элемента где у нас встречается текст $15.00,
 мы можем сформировать запрос //td[contains(text(), '$15.00')]. Если же мы хотим найти текст из второй строки из столбца с описанием, то //td[contains(text(), '$15.00')]
символ * используется для выбора всех элементов. Например, если мы хотим найти опять те же $15.00, то можем сделать запрос //*[@id="gift1"]/td[3]
не стоит забывать, что поиск по классу зависим от регистра и если мы будем искать класс написав его с большой буквы, а в документе он с маленькой,
 то элемент не будет найден на нашей странице.

Перед практикой выделим основные важные пункты при использовании XPath
Не стоит использовать селекторы вида //div[1]/div[2]/div[3] просто так, так как во первых непонятно, что мы ищем, во вторых при изменении структуры документа,
 наш код окажется неработоспособен
По возможности используйте css селекторы, такие как class, id, name, вместо XPath
Можно искать по полному или частичному совпадению текста или атрибута
Можно удобно перемещаться по структуре документа
Отлично подходит, если у сайта очень плохо указаны атрибуты

Использование XPATH к сожалению не возможно с beautifulsoup без использования сторонних библиотек.
 В большинстве своем с XPATH мы поработаем вместе с библиотекой selenium

Сейчас мы более подробнее рассмотрим requests и поработаем c bs4. Давайте попарсим какую нибудь страничку на таком сервисе, как Кинопоиск.
Создадим простой запрос и выведем тип возвращаемого объекта.
'''

def a7():
   import requests

   r = requests.get(url='https://www.kinopoisk.ru/media/article/4007433/’')
   print(type(r))



'''
В результате мы видим, что возвращается объект класса Response
Мы можем передавать этому классу и другие атрибуты помимо url
url - передает ссылку, куда будет отправлен запрос(обязательный)
params - словарь, который будет отправлен в строке запроса(необязательный)
headers - словарь HTTP-заголовков отправляемых запросом(необязательный)
cookies - объект Dict или CookieJar для отправки с запросом(необязательный)
auth - AuthObject для включения базовой аутентификации HTTP(необязательный)
timeout - число с плавающей точкой, описывающее тайм-аут запроса(необязательный)
allow-redirects - логическое значение разрешения перенаправления(необязательный)
proxies - протокол сопоставления словаря с URL-адресом прокси(необязательный)
stream - удержание соединения открытым, пока не получен весь контент

Со большинством этих параметром, мы познакомимся позже, по мере надобности.
 Так давайте теперь перенесемся на сайт и посмотрим его структуру с помощью консоли разработчика.
   Как мы видим весь основной контент находится в div  с классом media-article__body-wrapper.
    Давайте получим все данные из данного контейнера, но для начала нам надо проверить коды ответа HTTP.

Код ответа (состояния) HTTP показывает, был ли успешно выполнен определённый HTTP запрос. Коды сгруппированы в 5 классов:
Информационные 100 - 199
Успешные 200 - 299
Перенаправления 300 - 399
Клиентские ошибки 400 - 499
Серверные ошибки 500 - 599

Согласно данным кодам, мы должны получить код 200 если все в порядке.

'''
def a8():
   import requests

   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   print(r.status_code)




'''Как мы видим, код возврата 200, значит все отлично, также мы можем проверить с помощью raise_for_status().
 raise_for_status() возвращает объект HTTPError, если во время процесса произошла ошибка'''

def a9():
   import requests


   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   print(r.raise_for_status())


'''Из результата видно, что вернулось значение None, значит все в порядке. Мы будем использовать raise_for_status для проверки'''

def a10():
   import requests


   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
       pass


'''Импортируем bs4 и получим контент из необходимого блока'''
def a11():
   import requests
   import lxml
   from bs4 import BeautifulSoup


   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper')
      print(content)




'''Результат получился не очень красивый. Все в одну строчку. давайте отформатируем наш вывод с помощью метода prettify,
 который позволит сделать вывод более структурированным'''


def a12():
   import requests
   import lxml
   from bs4 import BeautifulSoup

   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper')
      print(content)
      print(content.prettify())

'''
Результат
<div class="media-article__body-wrapper">
 <div class="media-article__side">
  <div class="media-article__side-share-buttons">
 …
'''

'''Отлично теперь нам надо добраться до нужных статей по фильмам.
 Для этого мы можем посмотреть в консоли разработчика, что все статьи имеют один и тот же класс stk-grid stk-theme_27251__mb_cus_72'''
def a13():
   import requests
   import lxml
   from bs4 import BeautifulSoup


   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper')
      list_films = content.find_all('div', class_='stk-grid stk-theme_27251__mb_cus_72')
      print(list_films)




'''В результате find_all возвращает нам список найденных элементов. Теперь мы можем, либо перебрать весь список, либо обратиться к элементу по индексу'''
def a14():
   import requests
   import lxml
   from bs4 import BeautifulSoup


   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper')
      list_films = content.find_all('div', class_='stk-grid stk-theme_27251__mb_cus_72')
      article1 = list_films[0]
      print(article1.prettify()) # отфилтровали



'''
Теперь из примера выше мы спокойно можем получить, как заголовок, контент,  так и ссылки на сами статьи и изображения к первой статье.
 Начнем с заголовка. Если мы посмотрим на структуру страницы, то заголовки у нас хранятся в теге h2. Давайте получим его
…'''


def a15():
   import requests
   import lxml
   from bs4 import BeautifulSoup

   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper')
      list_films = content.find_all('div', class_='stk-grid stk-theme_27251__mb_cus_72')
      article1 = list_films[0]
      print(article1.prettify())
      article1 = list_films[0]
      title = article1.h2
      print(title)


'''Отлично! Заголовок получили, теперь нам нужно избавиться от тегов и получить только текст заголовка.Для этого мы можем пойти 2 вариантами.'''

def a16():
   import requests

   from bs4 import BeautifulSoup

   url = 'https://www.kinopoisk.ru/media/article/4007433/'
   r = requests.get(url=url)
   if not r.raise_for_status():
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper')
      list_films = content.find_all('div', class_='stk-grid stk-theme_27251__mb_cus_72')



      article1 = list_films[0]
      title = article1.h2.get_text()
      print(title)


'''либо же воспользоваться методом get_text(), который возвращает только текст
…'''



'''Отлично! Таким же методом теперь получим текст статьи, ссылку и ссылку на изображение, а также немного подкорректируем наш код,
 так как ссылки на статью возвращаются без указания самого сайта(например /film/1346357/)'''


def a17():
   import requests
   import lxml
   from bs4 import BeautifulSoup

   BASE_URL = 'https://www.kinopoisk.ru'
   url = f'{BASE_URL}/media/article/4007433/'

   r = requests.get(url=url)
   if not r.raise_for_status(): # если ошибки нет
      bs = BeautifulSoup(r.text, 'lxml')
      content = bs.find('div', class_='media-article__body-wrapper') #find- ищем 1 элемент \ findall-ищем вообще все
      list_films = content.find_all('div', class_='stk-grid stk-theme_27251__mb_cus_72')
      article1 = list_films[0]
      title = article1.h2.get_text() # получить текст
      content = article1.find('p', class_='stk-reset stk-theme_27251__mb_cus_36').get_text()
      article_url = article1.a.get('href')
      article_url = BASE_URL + article_url # обядиняиться ссылки
      image = article1.img.get('src')
      print(f'Заголовок {title}')
      print(content)
      print(article_url)#!!!!
      print(image)
a17()



'''
Конечно же данный результат можно сохранить, например в текстовый файл, но о возможности сохранения в файлы различных форматов, мы поговорим позднее.'''
