'''Урок 27.1 Углубленный анализ кода
Задачи урока:
Углубленный анализ кода

Столкнувшись с огромным количеством запутанных тегов, многие испытывают острое желание углубиться в них
с помощью многострочных операторов в расчете извлечь ценную информацию.
Однако следует помнить, что такой код будет трудно отлаживать, или он станет постоянно сбоить, или же и то и другое.
Предположим, мы ищем некий контент: имя, статистические данные или блок текста.
Возможно, этот контент закопан под 20 тегами в каше из HTML-кода и не отличается никакими особыми HTML-тегами или атрибутами.
Допустим, мы решили отбросить осторожность и написать примерно следующее в попытке извлечь нужные данные:
bs.find_all('table')[4].find_all('tr') [2].find('td').find_all('div')[1].find('a')
Выглядит не слишком красиво. Но дело не только в красоте: стоит администратору сайта внести малейшее изменение, весь наш веб-скрапер сломается.

Для начала нам требуется разобраться в структуре html документа.
Каждый сайт в интернете вы можете просмотреть не только в виде оформленной страницы с картинками,
но и в текстовом виде из которого мы и будем извлекать необходимые данные.
Первое что вам нужно знать - DOM это набор правил по которым формируется каждая страница в сети интернет используя язык разметки.
Хранит в себе структуру сайта в виде дерева и информацию заключенную в тегах

Каждый элемент дерева DOM называется узлом и  может иметь неограниченную вложенность.
Но не стоит делать их слишком глубокими чем больше вложенность тем сложнее разобраться в коде и тем медленнее работает страница.
Вложенность тегов HTML может напоминать вложенность обычных папок у вас на компьютере.
Отличие лишь в том что узел DOM имеет свое название атрибуты и свою цель.
К примеру тег дает браузеру понять какое название будет у вашей страницы на вкладке браузера как его нужно обработать и какие у него могут быть атрибуты

Пример структуры
<!DOCTYPE html>
<html lang="en">
<head>
   <meta charset="UTF-8">
   <title>Заголовок страницы</title>
</head>
<body>
   <div>
      <h1>Заголовок</h1>
      <p>Контент</p>
   </div>
</body>
</html>

Дерево помогает нам находить любой нужный элемент на странице и взаимодействовать с ним.
При написании парсеров не требуется что то делать с узлами. Необходимо только понимать его структуру и как получить доступ к конкретному
узлу и информации в нем.

HTML не является языком программирования, это язык разметки,
который используется чтобы сообщать вашему браузеру как нужно отображать веб страницы.
У каждой страницы есть ряд общих стандартных компонентов и набор уникальных тегов и атрибутов уровней вложенности и стилей.
Именно поэтому мы в целом с легкостью ориентируемся на любом сайте хотя они и не похожи друг на друга.
Сайты могут отображать видео проигрывать музыку и т.п. Обычно сайты состоят из тегов, которые хранятся в других тегах,
которые в свою очередь могут хранить сотни вложенных тегов внутри себя.
Зная и понимая как добраться до нужного тега мы можем получить нужную нам информацию для нашего парсера

Давайте рассмотрим примеры блочных и строчных элементов в html документе.
Блочные элементы составляют структуру страницы. К блочным тегам например относятся:
абзацы <p>
маркированные списки <ul> и нумерованные списки <ol>
заголовки от <h1> до <h6>
статьи <articles>
разделы <sections>
длинные цитаты <blockquote>
общие контейнеры <div>

Строчные элементы используются для форматирования текстовых фрагментов.
Пример:
ссылки <a>
выделенные слова <em>
слова выделенные, как важные <strong>
короткие цитаты <q>
аббревиатуры <abbr>

Давайте рассмотрим простой html документ и разберем его структуру, а также из чего состоит html элемент
<!DOCTYPE html>
<html lang="en">
<head>
   <meta charset="UTF-8">
   <title>Title</title>
</head>

<body>
   <div class="menu">
       <ul>
           <li><a href="">Парсинг</a></li>
           <li><a href="">Интерфейсы</a></li>
           <li><a href="">2D игры</a></li>
           <li><a href="">Телеграм боты</a> </li>
       </ul>
   </div>
   <div class="content">
       <h1>Заголовок статьи</h1>
       <p>Контент статьи</p>
   </div>
</body>

</html>

Первая конструкция в любом HTML-документе — элемент <!DOCTYPE>.
Он не относится к тегам и никаким образом не может отображаться на странице.
Его задача — указать браузеру, какой стандарт HTML используется в этом документе. Сейчас это везде стандарт HTML5.
Тег <html></html> является основным. Именно внутри него располагается вся информация.
Благодаря этому тегу браузер понимает, где начинается контент, который необходимо обработать как HTML.
Важной частью тега html является наличие атрибута lang. В нем указывается язык, на котором отображается веб-страница.
С помощью этого атрибута браузеры могут корректно считать множество специфичных символов, которые присутствуют в разных языках.

Тег <head></head> служит для хранения служебной информации. Здесь возможны самые разные сочетания тегов,
которые подсказывают браузеру название страницы, описание, ключевые слова и так далее.
Такая информация называется метаинформацией. Любые данные, которые указаны внутри тега <head>, не видны при отображении страницы в браузере.

Метатег <meta>. Он принимает множество разных атрибутов.
В настоящее время важным является метатег <meta> с атрибутом charset. Он позволяет установить кодировку документа.

Для указания заголовка страницы используется специальный парный тег <title></title>, внутри которого указывается нужная информация.

После тега <head> в документе указывается парный тег <body></body>, который является «телом» всей страницы.
Именно здесь размещается вся информация, которая будет выведена на странице.

Ранее мы рассмотрели установку и запуск BeautifulSoup, а также выбор объектов по одному.
Теперь мы обсудим поиск тегов по атрибутам, работу со списками тегов и навигацию по дереву синтаксического анализа.
Почти любой сайт, с которым вам придется иметь дело, содержит таблицы стилей.

На первый взгляд может показаться, что стили на сайтах предназначены исключительно для показа сайта пользователю в браузере. Однако это неверно.
Чтобы назначать элементам разные стили, CSS опирается на дифференциацию HTML-элементов, которые в противном случае имели бы одинаковую разметку.
Например, одни теги могут выглядеть так:
<span class="green"></span>
А другие — так:
<span class="red"></span>

Парсеры легко различают эти два тега по их классам, например, с помощью BeautifulSoup веб-скрапер может собрать весь красный текст, игнорируя зеленый.
Поскольку CSS использует эти идентифицирующие атрибуты для соответствующего оформления сайтов, мы можем быть практически уверены в том,
что на большинстве современных сайтов будет много атрибутов class и id.

Элементы на странице можно находить с помощью тегов, селекторов css, атрибутов и их значений.
id=’name_id’ - по идентификатору
class=’name_class’ - по имени класса
div - по имени тега
href=’#’ - по атрибуту
name=’item’ - по значению атрибута

Создадим пример веб-скрапера, который сканирует страницу, расположенную по адресу http://www.pythonscraping.com/pages/warandpeace.html.
На этой странице - строки, в которых содержатся реплики персонажей выделены красным цветом, а имена персонажей — зеленым.
В данном примере исходного кода страницы, показаны теги span, которым присвоены соответствующие классы CSS:
<span class="red">Heavens! what a virulent attack! </span>replied<span class=”green”> the prince</span>, not in the least disconcerted by this reception

С помощью программы, аналогичной той, которую мы написали ранее, можно прочитать всю страницу и создать на ее основе объект BeautifulSoup:'''
def a1():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
    bs = BeautifulSoup(html.read(), 'html.parser')
    print(bs)
#все целиком, всю странцу 




def a1_2():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup

    html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
    bs = BeautifulSoup(html.read(), 'html.parser')


    '''С помощью этого объекта BeautifulSoup можно вызвать функцию find_all и извлечь Python-список всех имен персонажей, полученных путем выбора текста из тегов
    <span class = 'green'>'''
    nameList = bs.find_all('span', {'class': 'green'})
    for name in nameList:
        print(name.get_text())

a1_2()
'''Результатом выполнения этого кода должен стать список всех персонажей «Войны и мира» в порядке их появления в тексте.

Так что же здесь происходит? Раньше мы вызывали функцию bs.имяТега и получали первое появление тега на странице.
Теперь мы вызываем функцию s.find_all(имяТега,атрибутыТега), чтобы получить не только первый тег, а список всех тегов,
присутствующих на странице. Получив список персонажей, программа перебирает все имена в списке и использует функцию name.get_text(), чтобы очистить контент от тегов.

Функция .get_text() удаляет из документа, с которым вы работаете, все теги и возвращает строку,
содержащую только текст в кодировке Unicode. Например, при работе с большим блоком текста, содержащим много гиперссылок,
абзацев и других тегов, все эти теги будут удалены, останется только текст.
Учтите, что в объекте BeautifulSoup гораздо проще найти нужное, чем в текстовом фрагменте.
Вызов .get_text() всегда должен быть последним, что вы делаете непосредственно перед выводом результата на экран,
сохранением или манипулированием готовыми данными. Как правило, следует как можно дольше сохранять структуру тегов документа.

Функции BeautifulSoup find() и find_all() вы, скорее всего, будут использоваться чаще других.
С помощью этих функций можно легко фильтровать HTML-страницы, чтобы выделить списки нужных тегов или найти отдельный тег
по всевозможным атрибутам. Эти две функции очень похожи:
find_all(tag, attributes, recursive, text, limit, keywords)
find(tag, attributes, recursive, text, keywords)

Скорее всего, в 95 % случаев вы будете использовать только первые два аргумента: tag и attributes.
Но мы постараемся рассмотреть все аргументы. Аргумент tag нам уже встречался, мы можем передать функции строку,
содержащую имя тега, или даже Python список имен тегов. Например, следующий код возвращает список всех тегов заголовков, встречающихся в документе :
 .find_all(['h1','h2','h3','h4','h5','h6'])

Аргумент attribute принимает Python-словарь атрибутов и ищет теги, которые содержат любой из этих атрибутов.
Например, следующая функция ищет в HTML-документе теги span с классом green или red:
.find_all('span', {'class':{'green', 'red'}})

Аргумент recursive - логический. Насколько глубоко вы хотите исследовать документ?
Если recursive присвоено значение True, то функция find_all ищет теги, соответствующие заданным параметрам, в дочерних элементах и их потомках.
Если же значение этого аргумента равно False, то функция будет просматривать только теги верхнего уровня документа.
По умолчанию find_all работает рекурсивно (recurive имеет значение True); обычно лучше оставить все как есть, за исключением ситуаций,
когда вы точно знаете, что делаете, и нужно обеспечить высокую производительность.

Аргумент text - необычен из-за отношения не к свойствам тегов, а к их текстовому контенту.
Так, чтобы узнать, сколько раз на странице встречается слово the prince, заключенное в теги, можно заменить функцию .find_all() из предыдущего примера на следующие строки:
nameList = bs.find_all(text='the prince')
print(len(nameList))

Результатом будет число 7.

Аргумент limit по понятным причинам используется только в методе find_all. Функция find эквивалентна вызову find_all со значением limit, равным 1.
Этот аргумент можно использовать в тех случаях, когда вы хотите извлечь только первые x элементов, присутствующих на странице.
Однако следует учитывать, что вы получите первые элементы в порядке их появления на странице, и это вовсе не обязательно будут те элементы, которые вам нужны.

Аргумент keyword позволяет выбрать теги, содержащие определенный атрибут или набор атрибутов. Например:
title = bs.find_all(id='title', class_='text')
Этот код возвращает первый тег со словом text в атрибуте class_ и словом title в атрибуте id.

До сих пор нам встречались два типа объектов библиотеки BeautifulSoup:
• объектыBeautifulSoup — экземпляры, которые в предыдущих примерах кода встречались в виде переменной bs;
• объектыTag — в виде списков или отдельных элементов, как результаты вызовов функций find и find_all для объекта BeautifulSoup
или полученные при проходе по структуре объекта BeautifulSoup:
bs.div.h1

Однако в библиотеке есть еще два объекта, которые используются реже, но все же о них важно знать:
• объектыNavigableString — служат для представления не самих тегов, а текста внутри тегов (некоторые функции принимают и создают не объекты тегов, а объекты NavigableString);
• объектыComment — применяются для поиска HTML-комментариев, заключенных в теги комментариев, <!--например,так-->.

Из всей библиотеки BeautifulSoup вам в большинстве случаев придется иметь дело только с этими четырьмя объектами.

Функция find_all выполняет поиск тегов по их именам и атрибутам. Но как быть, если нужно найти тег по его расположению в документе?
Здесь нам пригодится навигация по дереву. Ранее мы встречали навигацию по дереву BeautifulSoup только в одном направлении:
bs.tag.subTag.anotherSubTag

Теперь рассмотрим навигацию по деревьям HTML-кода во всех направлениях: вверх, по горизонтали и диагонали.
В качестве образца для веб-скрапинга мы будем использовать
тестовый интернет-магазин, размещенный по адресу http://www.pythonscraping.com/pages/page3.html.

HTML-код этой страницы, представленный в виде дерева (некоторые теги для краткости опущены), выглядит так:
HTML
— body
    — div.wrapper
         — h1
         — div.content
         — table#giftList
             — tr
                   — th
                   — th
                   — th
                   — th
          — tr.gift#gift1
                   — td
                   — td
                         — span.excitingNote
                  — td
                  — td
                        — img
          — ...другие строки таблицы...
   — div.footer

В BeautifulSoup, как и во многих других библиотеках, существует различие между детьми и потомками:
- как и в генеалогическом древе любого человека, дети всегда располагаются ровно на один уровень ниже родителей,
тогда как потомки могут находиться на любом уровне дерева ниже родителя. Скажем, теги tr являются детьми тега table, а теги
tr, th, td, img и span — потомками тега table (по крайней мере, в нашем примере). Все дети — потомки, но не все потомки — дети.

В целом функции BeautifulSoup всегда имеют дело с потомками тега, выбранного в данный момент.
Например, функция bs.body.h1 выбирает первый тег h1, который является потомком тега body. Она не найдет теги, расположенные за пределами body.
Аналогично функция bs.div.find_all('img') найдет первый тег div в документе, а затем извлечет список всех тегов img, которые являются потомками этого тега div.
Получить только тех потомков, которые являются детьми, можно с помощью тега .children:'''

def a3():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    html = urlopen('https://www.pythonscraping.com/pages/page3.html')
    bs = BeautifulSoup(html, 'html.parser')
    for child in bs.find('table',{'id':'giftList'}).children:
       print(child)

'''Данный код выводит список всех строк таблицы giftList, в том числе начальную строку с заголовками столбцов.
Если вместо функции children() в этом коде использовать функцию desndants(),
то она найдет в таблице и выведет примерно два десятка тегов, включая img, span и отдельные теги td.

Функция next_siblings() библиотеки BeautifulSoup упрощает сбор данных из таблиц, особенно если в таблице есть заголовки:'''

def a4():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    html = urlopen('https://www.pythonscraping.com/pages/page3.html')
    bs = BeautifulSoup(html, 'html.parser')
    for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:
       print(sibling)


'''Этот код должен выводить все строки таблицы, кроме первой с заголовком. Почему пропускается строка заголовка? Потому что объект не может быть сиблингом сам себе.
Каждый раз, когда составляется список сиблингов (братьев и сестер) объекта, сам объект не включается в этот список. Как следует из названия,
данная функция выбирает только следующих по списку сиблингов. Например, если выбрать строку,
 расположенную в середине таблицы, и вызвать для нее функцию next_siblings, то функция вернет только тех сиблингов,
которые идут в списке после данной строки.
Таким образом, выбрав строку заголовка и вызвав функцию next_siblings, мы получим все строки таблицы, кроме самой строки заголовка.
У функции next_siblings есть парная функция previous_siblings. Она часто бывает полезна, если в конце списка одноуровневых тегов, который вы хотели бы получить,
есть легко выбираемый тег.
И конечно же, существуют функции next_sibling и previous_sibling, которые выполняют почти то же, что и next_siblings и previous_siblings,
но только возвращают не список тегов, а лишь один тег.

При сборе данных со страниц вы, скорее всего, быстро поймете, что выбирать родительский тег необходимо реже, чем детей или сиблингов.
Как правило, просмотр HTML-страницы с целью поиска данных мы начинаем с тегов верхнего уровня, после чего ищем способ углубиться в нужный фрагмент данных.
Однако иногда встречаются странные ситуации, когда приходится использовать функции поиска родительских элементов .parent и .parents из библиотеки BeautifulSoup.'''

def a5():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup

    html = urlopen('https://www.pythonscraping.com/pages/page3.html')
    bs = BeautifulSoup(html, 'html.parser')
    print(bs.find('img', {'src': '../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())

'''До сих пор мы исследовали способы доступа к тегам и их контенту и способы их фильтрации. Однако часто при парсинге нас интересует не содержимое тега, а его атрибуты.
Это особенно полезно для таких тегов, как a, в атрибуте href содержащих URL, на которые ссылаются эти теги, или же тегов img, в атрибуте src содержащих ссылки на целевые
изображения.

Python позволяет автоматически получить список атрибутов для объекта тега, вызвав следующую функцию:
myTag.attrs
Эта функция возвращает в чистом виде словарь Python, благодаря чему получение атрибутов и управление ими становится тривиальной задачей.
Например, для того, чтобы узнать, где находится файл с изображением, можно воспользоваться следующим кодом:
myImgTag.attrs['src']

В Beautiful Soup можно применять также и регулярные выражения.
На них мы особо заострять внимание не будем, но кому интересно обязательно обратите внимание на модуль re стандартной библиотеки Python.'''
def a6():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    import re
    html = urlopen('https://www.pythonscraping.com/pages/page3.html')
    bs = BeautifulSoup(html, 'html.parser')
    images = bs.find_all('img', {'src':re.compile('..\/img\/gifts/img.*.jpg')})
    for image in images:
       print(image['src'])


'''К сожалению, изучение регулярных выражений (часто сокращаемых до regex) часто сводится к пространным таблицам случайных символов, сочетания которых выглядят
как абракадабра. Это отпугивает многих людей, а потом они решают рабочие задачи и пишут ненужные сложные функции поиска и фильтрации,
хотя можно было бы обойтись всего одной строкой с регулярным выражением!
К счастью, регулярные выражения не так уж трудно быстро освоить.
Чтобы их изучить, достаточно рассмотреть всего несколько простых примеров и поэкспериментировать с ними.

Регулярные выражения получили свое название благодаря тому, что используются для идентификации регулярных строк.
Регулярное выражение позволяет сделать однозначный вывод о том, что данная строка соответствует определенным правилам,
и вернуть ее или же сделать вывод о том, что строка не соответствует правилам, и отбросить ее. Это невероятно
удобно для быстрой проверки больших документов, в которых нужно найти номера телефонов или адреса электронной почты.
Мы использовали выражение «регулярная строка». Что это такое? Это любая строка, которую можно построить с учетом последовательности линейных правил 3 следующего вида.
1. Написать хотя бы одну букву a.
2. Добавить ровно пять букв b.
3. Добавить произвольное четное число букв c.
4. В конце поставить букву d или e.
Этим правилам соответствуют строки
 aaaabbbbbccccd,
aabbbbbcce и т.д. (количество вариантов бесконечно).

Регулярные выражения — всего лишь краткий способ представления этих наборов правил. Например, регулярное выражение для описанного выше набора правил выглядит так:
aa*bbbbb(cc)*(d|e)
На первый взгляд эта строка смотрится жутковато, но она станет понятнее, если разбить ее на составляющие:
•aa* — буква a, после которой стоит символ * (звездочка), означает «любое количество букв a, включая 0».
Такая запись гарантирует, что буква a будет написана хотя бы один раз;
• bbbbb — ничего особенного, просто пять букв b подряд;
• (cc)* — любое количество чего угодно можно заключить в скобки. Поэтому для реализации правила о четном количестве букв c мы можем написать две буквы c,
 заключить их в скобки и поставить после них звездочку. Это значит, что в строке может присутствовать любое
количество пар, состоящих из букв c (обратите внимание, что это также может означать 0 пар);
• (d|e) — вертикальная линия между двумя выражениями означает «то или это». В данном случае мы говорим «добавить d или e».
 Таким образом мы гарантируем, что в строку добавится ровно один из этих двух символов.


1
^ — соответствует началу строки.
2
$— соответствует концу строки.
3
. — соответствует любому символу, кроме новой строки. Использование флага re.M позволяет также соответствовать новой строке.
4
[4fw] — соответствует любому из символов в скобках.
5
[^4fw] — соответствует любому символу, кроме тех, что в квадратных скобках.
6
foo* — соответствует 0 или более вхождений “foo”.
7
bar+ —- соответствует 1 или более вхождениям “bar”.
8
foo? —- соответствует 0 или 1 вхождению “foo”.
9
bar{3} —- соответствует трем подряд вхождениям “bar”.
10
foo{3,} — соответствует 3 или более вхождениям “foo”.
11
bar{2,5} —- соответствует от 2 до 5 вхождениям “bar”.
12
a|b — соответствует либо a, либо b.
13
(foo) — группирует регулярные выражения.
14
(?imx) — временно включает параметры i, m или x в регулярное выражение. Если используются круглые скобки — затрагивается только эта область.
15
(?-imx) — временно отключает опции i, m или x в регулярном выражении. Если используются круглые скобки — затрагивается только эта область.
16
(?: foo) — Группирует регулярные выражения без сохранения совпадающего текста.
17
(?imx: re) — Временно включает параметры i, m или x в круглых скобках.
18
(?-imx: re) — временно отключает опции i, m или x в круглых скобках.
19
(?#…) — комментарий.
20
(?= foo) — совпадает со всеми словами после которых » foo».
21
(?! foo) — совпадает со всеми словами после которых нет » foo».
22
(?> foo) — совпадает со всеми словами перед которыми » foo».
23
\w — совпадает с буквенным символом.
24
\W — совпадает с не буквенным символом.
25
\s — совпадает с пробельными символами (\t, \n, \r, \f и пробелом).
26
\S — все кроме пробельных символов.
27
\d — соответствует цифрам (0-9).
28
\D — все кроме цифры.
29
\A — соответствует началу строки.
30
\Z – соответствует концу строки. Включая перевод на новую строку, если такая есть.
31
\z — соответствует концу строки.
32
\G — соответствует месту, где закончилось последнее соответствие.
33
\b — соответствует границам слов, когда поставлены внешние скобки.
34
\B — все кроме границы слова.
35
**\n,\t,\r,\f ** — соответствует новым строкам, подстрокам.
36
\1…\9 — соответствует подгруппе n-й группы.
37
\10 — соответсвуйет номеру группы. В противном случае относится к восьмеричному представлению символьного кода.


В отношении парсинга BeautifulSoup и регулярные выражения идут рука об руку.
В сущности, функция, принимающая строку в качестве аргумента (например, find(id="идентификаторТега")), скорее всего, будет принимать и регулярное выражение.

Рассмотрим несколько примеров, проверив страницу по адресу http://www.python scraping.com/pages/page3.html.
На этом сайте есть много изображений товаров, представленных в таком виде:
<img src="../img/gifts/img3.jpg">

Если мы хотим собрать URL всех изображений товаров, то на первый взгляд решение может показаться довольно простым:
достаточно выбрать все теги изображений с помощью функции .find_all("img"), верно? Не совсем.

Кроме очевидных «лишних» изображений (например, логотипов), на современных сайтах часто встречаются скрытые и пустые изображения, используемые вместо пробелов и для
выравнивания элементов, а также другие случайные теги изображений, о которых вы, возможно, не знаете. Определенно нельзя рассчитывать на то,
что все изображения на странице являются только изображениями товаров.
Предположим также, что макет страницы может изменяться или по какой-либо причине поиск правильных тегов не должен зависеть от расположения изображений на странице.
Например, вы хотите собрать определенные элементы или фрагменты данных, разбросанные по всему сайту случайным образом.
Так, для титульного изображения товара может быть предусмотрено специальное место наверху некоторых — но не всех — страниц.

Решение состоит в поиске чего-то идентифицирующего сам тег. В данном случае можно поискать путь к файлам изображений товаров'''
def a7():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup
    import re
    html = urlopen('http://www.pythonscraping.com/pages/page3.html')
    bs = BeautifulSoup(html, 'html.parser')
    images = bs.find_all('img', {'src':re.compile('..\/img\/gifts/img.*.jpg')})
    for image in images:
       print(image['src'])

'''Этот код выводит только те относительные пути к изображениям, которые начинаются с ../img/gifts/img и заканчиваются на .jpg
Регулярное выражение может использоваться как аргумент в выражении, написанном на BeautifulSoup, что обеспечивает большую гибкость при поиске нужных элементов.

Домашняя работа

Задача 1
Спарсить ссылку на изображение с главной страницы https://www.python.org/, а также текст из div с классом psf-widget'''













