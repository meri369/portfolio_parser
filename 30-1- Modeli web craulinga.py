"""Методические указания
Урок 30.1 Модели веб-краулинга
Задачи урока:
Модели веб-краулинга


1. Вели веб-краулинга

Писать чистый и масштабируемый код довольно сложно,
даже если у вас есть возможность контролировать входные и выходные данные.
Написание кода веб-краулера, которому иногда приходится выполнять веб-скрапинг и сохранять различные данные
 с разных групп сайтов никак не контролируемых программистом часто представляет собой
невероятно сложную организационную задачу.
Вам могут предложить собрать новостные статьи или публикации из блогов, размещенных на различных сайтах,
 у каждого из которых свои шаблоны и макеты.
  На одном сайте ег h1 может содержать заголовок статьи, а на другом —заголовок самого сайта,
  а заголовок статьи будет заключен в тег <span id="title">.
Возможно, вам понадобится гибко управлять тем, для каких сайтов нужно выполнить веб-скрапинг и как именно это делать,
 а также способ быстро добавлять новые сайты или изменять существующие — максимально быстро и без необходимости писать много строк кода.

Вас могут попросить собрать цены на товары с разных сайтов, чтобы в итоге можно было сравнивать цены на один и тот же товар.
Возможно, они будут представлены в разных
валютах.
 Вдобавок, вероятно, потребуется объединить их с внешними данными, полученными из какого-то другого источника,
не имеющего отношения к Интернету.
Несмотря на то что вариантов применения веб-краулеров бесчисленное множество, большие масштабируемые веб-краулеры,
 как правило, относятся к одному из нескольких
типов. Исследуя эти типы и распознавая ситуации, в которых они используются,
 можно значительно повысить удобство сопровождения и надежность своих веб-краулеров.
Мы уделим основное внимание веб-краулерам, собирающим ограниченное количество «типов» данных
(таких как обзоры ресторанов, новостные статьи, профили компаний)
с большого количества сайтов и хранящим эти данные в виде объектов Python, которые читают и записывают в базу данных.

Одна из распространенных ловушек веб-скрапинга —определение данных, которые разработчики намерены собирать,
 исключительно на основании того,
  что есть у них перед глазами.
Например, желая собрать данные о товаре, можно было бы для начала заглянуть в магазин одежды и решить,
 что для каждого интересующего нас товара
необходимо получить данные из следующих полей:
• наименование товара;
• цена;
• описание;
• размеры;
• цвета;
• тип ткани;
• рейтинг клиентов.
Посмотрев на другой сайт, вы обнаружите, что у товара есть
SKU (stock keeping unit, единицы хранения, или артикул,используемый для отслеживания и заказа товаров).
 Вы наверняка захотите собирать и эти данные, даже если их не было на первом сайте! И вы добавите в список данное поле:
• артикул.
Для начала одежда вполне подойдет, однако необходимо убедиться, что этот веб-скрапер можно будет распространить и на другие типы товаров.
 Начав просматривать разделы товаров на других сайтах, вы поймете, что также необходимо собрать следующую информацию:
• твердая/мягкая обложка;
• матовая/глянцевая печать;
• количество отзывов клиентов;
• ссылка на сайт производителя.

Понятно, что такой подход неприемлем. Просто добавляя атрибуты к типу товара всякий раз,
когда на сайте встречается новая информация, вы получите слишком большое количество полей, которые нужно отслеживать.
Мало того, каждый раз при веб-скрапинге нового сайта придется подробно анализировать поля, уже имеющиеся на нем, и те,
что уже были накоплены ранее, а также, возможно, добавлять новые (изменяя тип

объекта Python и структуру базы данных). Это приведет к запутанному и трудночитаемому набору данных, а следовательно,
к проблемам при его использовании.
Принимая решение о том, какие данные собирать, зачастую лучше игнорировать сайты. Нельзя запустить проект,
 рассчитанный на то, чтобы стать большим и масштабируемым,
посмотрев только на один сайт и спросив себя: «Что здесь есть?» Вместо этого нужно задать другой вопрос:
«Что мне нужно?» — а затем найти способы поиска необходимой
информации.
Возможно, в действительности вам нужно всего лишь сравнивать цены на товары в нескольких магазинах и
отслеживать изменения этих цен с течением времени.
 В этом
случае вам необходима только та информация, которая бы позволила однозначно идентифицировать товар:
• название товара;
• производитель;
• идентификационный номер товара
Важно отметить, что ни одно из этих свойств товара не относится к конкретному магазину.

Например, обзоры товаров, рейтинги, цены и даже описания одного и того же товара могут
отличаться в разных магазинах, и их стоит хранить отдельно.
Другие данные (цветовые варианты товара и материал, из которого он сделан) относятся к товару,
 но могут быть разреженными — существовать не для всех единиц товара.

Важно сделать шаг назад, составить контрольный список для каждого свойства товара, которое вы считаете необходимым,
и задать себе следующие вопросы.
• Поможет ли данная информация достичь целей проекта? Зайду ли я в тупик, если не получу ее,
или это лишь данные из разряда «пригодится», но, по большому счету, они ни на
что не влияют?

• Если эти данные когда-нибудь могут пригодиться (а могут и нет) — насколько сложно будет вернуться и собрать их?
• Являются ли эти данные избыточными относительно уже собранных?
• Имеет ли логический смысл хранить данные именно в этом объекте? (Как уже упоминалось,

сохранение описания в качестве свойства товара не имеет смысла, если описание одного и того же товара может быть разным на разных сайтах.)
Определившись, какие данные нужно собрать, важно задать себе еще несколько вопросов,
чтобы затем решить, как хранить и обрабатывать эти данные в коде.
• Эти данные разреженные или плотные? Свойственны ли они любому товару,
 в любом списке или же существуют только для небольшого множества товаров?
• Насколько велик объем данных?
• Особенно это касается больших данных: нужно ли будет регулярно получать их каждый раз при выполнении анализа и
ли только в отдельных случаях?

• Насколько переменчивы данные этого типа? Придется ли регулярно добавлять новые атрибуты, изменять типы?
Как мы видим, что для подготовки к написанию нашего парсера, нам необходимо перекопать и переработать множество информации.

Одно из наиболее впечатляющих достижений поисковых систем, таких как Google,
состоит в возможности извлекать релевантные и полезные данные с различных сайтов, не имея
предварительных знаний об их структуре. Мы, люди, способны с первого взгляда определить,
 где у страницы заголовок, а где — основной контент (за исключением случаев крайне плохого веб-дизайна),
  однако заставить бот делать то же самое гораздо труднее.

К счастью, в большинстве случаев при веб-краулинге мы не намерены собирать данные с сайтов,
 которые никогда прежде не видели. Обычно речь идет максимум о нескольких десятках сайтов,
  предварительно отобранных человеком. Это значит,  что нам не понадобятся сложные алгоритмы или машинное обучение для определения того,
   какой текст на странице «больше всего похож на заголовок», а какой, скорее всего, является «основным контентом».

    Все эти элементы можно задать вручную.
Наиболее очевидный подход — написать отдельный веб- краулер или парсер страниц для каждого сайта.
 Каждый такой краулер может принимать URL, строку или объект BeautifulSoup и возвращать результат веб-скрапинга в виде объекта Python.

Напишем пример класса Content (представляющего собой фрагмент контента сайта, например новостную публикацию) и две функции веб-скрапинга,
 которые принимают объект BeautifulSoup и возвращают экземпляр Content:
"""
import requests
from bs4 import BeautifulSoup


class Content:
   def __init__(self, url, title, body):
       self.url = url
       self.title = title
       self.body = body


def getPage(url):
   r = requests.get(url)
   return BeautifulSoup(r.text, 'lxml')


def parseNYTimes(url):
   bs = getPage(url)
   title = bs.find('h1').text
   lines = bs.find('article', {'class': 'live-blog-content meteredContent css-1napa12'})
   body = '\n'.join([line.text for line in lines])
   return Content(url, title, body)


def parseBrooking(url):
   bs = getPage(url)
   title = bs.find('h1').text
   body = bs.find('div', {'class', 'post-body'}).text
   return Content(url, title, body)


url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'
content = parseBrooking(url)
print(f'Заголовок: {content.title}')
print(f'URL: {content.url}')
print(content.body)




"""
Добавляя функции веб-скрапинга для других новостных сайтов, вы, вероятно, заметите некоторую закономерность. 
В сущности, функция синтаксического анализа любого сайта
делает одно и то же:
• находит элемент заголовка и извлекает оттуда текст заголовка;
• находит основной контент статьи;
• при необходимости находит другие элементы контента;
• возвращает объект Content, созданный с помощью ранее найденных строк.

Вспомним наш класс Content.
 Мы поняли, что функция анализа любого сайта имеет много общего и мы можем пользоваться некими шаблонами. 
 Единственное, что здесь действительно зависит от сайта, это CSS-селекторы, используемые для получения каждого элемента информации.
  Функции BeautifulSoup find и find_all принимают два аргумента: строку тега и словарь атрибутов в формате 
  «ключ — значение», вследствие чего эти аргументы можно передавать как параметры,
   которые определяют структуру сайта и расположение нужных данных.
Чтобы было еще удобнее, вместо аргументов тегов и пар «ключ — значение» можно использовать функцию BeautifulSoup select,
 принимающую строку CSS-селектора для
каждого элемента информации, который вы хотите получить, и разместить все эти селекторы в словарном объекте:
"""
class Content:
   """
   Общий родительский класс для всех статей/страниц.
   """

   def __init__(self, url, title, body):
       self.url = url
       self.title = title
       self.body = body

   def print(self):
       """
       Гибкая функция печати, управляющая выводом
       данных.
       """
       print('URL: {}'.format(self.url))
       print('TITLE: {}'.format(self.title))
       print('BODY:\n{}'.format(self.body))


class Website:
   """
   Содержит информацию о структуре сайта.
   """

   def __init__(self, name, url, titleTag, bodyTag):
       self.name = name
       self.url = url
       self.titleTag = titleTag
       self.bodyTag = bodyTag



"""
Обратите внимание: в классе Website хранится не информация, собранная с разных страниц, а инструкции о том,
 как ее собирать. Так, здесь хранится не заголовок «Название
моей страницы», а лишь строка с тегом h1, который указывает на то, где содержатся заголовки. 
Именно поэтому класс называется Website (его информация относится ко всему
сайту), а не Content (в котором содержится информация только с одной страницы).
С помощью классов Content и Website можно написать класс Crawler для веб-скрапинга заголовка и контента,
 размещенных по любому URL веб-страницы, принадлежащей
данному сайту:
"""
import requests
from bs4 import BeautifulSoup


class Content:
   def __init__(self, url, title, body):
       self.url = url
       self.title = title
       self.body = body

   def print(self):
       print(f'Заголовок: {self.title}')
       print(f'URL: {self.url}')
       print(self.body)


class Website:
   def __init__(self, name, url, titleTag, bodyTag):
       self.name = name
       self.url = url
       self.titleTag = titleTag
       self.bodyTag = bodyTag


class Crawler:
   def getPage(selfself, url):
       try:
           r = requests.get(url)
       except requests.exceptions.RequestException:
           return None
       return BeautifulSoup(r.text, 'lxml')

   def safeGet(self, pageObj, selector):
       selectedElems = pageObj.select(selector)
       if selectedElems is not None and len(selectedElems) > 0:
           return '\n'.join([elem.get_text() for elem in selectedElems])
       return ''

   def parse(self, site, url):
       bs = self.getPage(url)
       if bs is not None:
           title = self.safeGet(bs, site.titleTag)
           body = self.safeGet(bs, site.bodyTag)
           if title != '' and body != '':
               content = Content(url, title, body)
               content.print()


crawler = Crawler()
siteData = [
   ['Brookings', 'https://www.brookings.edu', 'h1', 'div.post-body']
]
websites = []
for row in siteData:
   websites.append(Website(row[0], row[1], row[2], row[3]))
crawler.parse(websites[0],
             'https://www.brookings.edu/blog/future-development/2022/03/04/how-to-close-europes-digital-divide/')



"""
На первый взгляд этот новый способ может показаться удивительно простым,
 по сравнению с написанием отдельной функции Python для каждого сайта. Однако представьте, что
произойдет при переходе от системы с четырьмя сайтами-источниками к системе с 20 или 200 источниками.
 Написать список строк относительно легко, и он не займет много места. 
 Такой список можно загрузить из базы данных или CSV-файла, импортировать из удаленного источника или передать кому-то,
  кто не является программистом, но умеет
заполнять формы и вводить новые сайты через пользовательский интерфейс, и этот человек никогда не увидит ни одной строки кода.

Конечно, здесь есть недостаток: мы отказываемся от определенной гибкости.
 В первом примере у каждого сайта есть собственная функция, написанная в свободной форме, для
выбора и — если получение результата того требует — синтаксического анализа HTML-кода. 

Во втором примере все сайты должны иметь схожую структуру, в которой гарантированно существуют определенные поля. 
Полученные из них данные должны быть чистыми, а каждому интересующему нас полю следует иметь уникальный и надежный CSS-селектор.
Однако я считаю, что широкие возможности и относительная гибкость данного подхода более чем компенсируют его реальные
 или предполагаемые недостатки.
В следующем разделе мы рассмотрим конкретные способы применения и варианты расширения этой базовой схемы, которые позволят,
 например, справляться с отсутствующими полями, собирать различные типы данных, проверять только
определенные части сайта и хранить более сложную информацию о страницах.

Создание гибких и изменяемых типов разметки сайтов не принесет особой пользы, если все равно придется вручную искать каждую ссылку,
 чтобы выполнить для нее веб-скрапинг.
Мы с вами рассмотрим, как встроить эти методы в хорошо структурированный и расширяемый веб-краулер, который бы
 автоматически собирал ссылки и находил нужные
данные. Мы разберем только три основные структуры веб-краулера:
Веб-краулинг с помощью поиска
Сбор данных с сайтов по ссылкам
Сбор данных со страниц нескольких типов

Один из самых простых способов сбора данных с сайта — тот же, что используют люди: с помощью панели поиска. 
Поиск на сайте по ключевому слову или теме и последующий сбор
данных по списку результатов может показаться задачей, весьма зависящей от конкретного сайта, 
однако несколько ключевых моментов делают ее на удивление примитивной.
• Большинство сайтов получают список результатов поиска по определенной теме, передавая ее в виде строки через параметр URL,
 например: http://example.com?search=мояТема. Первую часть этого URL можно сохранить как свойство объекта Website и потом просто каждый
  раз добавлять к нему тему.
  
• Выполнив поиск, большинство сайтов формируют страницы результатов в виде легко идентифицируемого списка ссылок, 
обычно заключенных в удобный тег наподобие
<spanclass="result">, точный формат которого тоже можно сохранить в виде свойства объекта Website.
• Каждая ссылка на результат поиска представляет собой либо относительный URL (такой как /articles/page.html), 
либо абсолютный (например, http://example.com/articles/page.html). Независимо от того, какой вариант вы ожидаете — 
абсолютный или относительный, — URL можно сохранить как свойство объекта Website.
• Найдя и нормализовав URL на странице поиска, мы успешно сводим задачу к примеру, рассмотренному ранее, — 
извлечению данных со страницы сайта,  имеющей заданный формат.
"""


import requests
from bs4 import BeautifulSoup


class Content:
   def __init__(self, topic, url, title, body):
       self.topic = topic
       self.url = url
       self.title = title
       self.body = body

   def print(self):
       print(f'Новая тема {self.topic}')
       print(f'URL {self.url}')
       print(f'Заголовок {self.title}')
       print(f'\n{self.body}')


class Website:
   def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):
       self.name = name
       self.url = url
       self.searchUrl = searchUrl
       self.resultListing = resultListing
       self.resultUrl = resultUrl
       self.absoluteUrl = absoluteUrl
       self.titleTag = titleTag
       self.bodyTag = bodyTag


class Crawler:
   def getPage(self, url):
       try:
           r = requests.get(url)
       except requests.exceptions.RequestException:
           return None
       return BeautifulSoup(r.text, 'lxml')

   def safeGet(self, pageObj, selector):
       selectedItem = pageObj.select(selector)

       if selectedItem is not None and len(selectedItem) > 0:
           return selectedItem[0].get_text()
       return ''

   def search(self, topic, site):
       bs = self.getPage(site.searchUrl + topic)
       searchResults = bs.select(site.resultListing)
       for result in searchResults:
           url = result.select(site.resultUrl)[0].attrs['href']
           if (site.absoluteUrl):
               bs = self.getPage(url)
           else:
               bs = self.getPage(site.url + url)
           if bs is None:
               print('Ошибка')
               return
           title = self.safeGet(bs, site.titleTag)
           body = self.safeGet(bs, site.bodyTag)
           if title != '' and body != '':
               content = Content(topic, title, body, url)
               content.print()
           else:
               return


crawler = Crawler()
siteData = [
   ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=', 'div.list-content article',
    'h4.title a', True, 'h1', 'div.post-body']

]
websites = []
for row in siteData:
   websites.append(Website(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]))

topics = ['AI']
for topic in topics:
   print(f'Информация по теме {topic}')
   for targetSite in websites:
       crawler.search(topic, targetSite)




"""
Этот скрипт перебирает все темы из списка topics и, прежде чем приступить к веб-скрапингу по очередной теме, выводит:
Информация по теме python
Затем скрипт просматривает все сайты из списка sites и сканирует каждый из них по каждой теме.
 Всякий раз, успешно находя информацию о странице, скрипт выводит ее в консоль



Домашняя работа
Переделать свой парсер с использованием моделей





"""

