import requests
from bs4 import BeautifulSoup


# Ну что ж пора переходить к изучению парсинга. С первых шагов веб-скрапинга вы сразу начнете ценить все те мелкие услуги,
# которые нам оказывают браузеры. Без HTML форматирования, стилей CSS,
# скриптов JavaScript и рендеринга изображений Интернет с непривычки может показаться слегка запутанным.На дальнейших занятиях мы посмотрим,
# как форматировать и интерпретировать данные, не прибегая к помощи браузера.
# Мы начнем с основ отправки на веб-сервер GET-запросов — запросов на выборку или получение содержимого заданной веб-страницы,
# чтения полученного с нее HTML-кода и выполнения ряда простых операций по извлечению данных, чтобы выделить оттуда контент, который вы ищете.
# Когда мы переходим на какой либо сайт, браузер получает html страницу и выводит нам их на экран,
# но то же самое можно сделать и на Python с помощью всего трех строк кода:


def a1():
    from urllib.request import urlopen

    url = 'https://ru.wikipedia.org/wiki/Python'
    html = urlopen(url)
    result = html.read()

# В данном случае мы используем модуль urllib из стандартной библиотеки python,
# а именно функцию urlopen, которая принимает от нас один обязательный параметр url(адрес страницы).
# У данной функции есть и необязательные параметры, но они нам пока не интересны.
# Во второй строке мы открываем необходимую страницу и сохраняем все данные в переменной html. Ну и после с помощью метода read(), мы считываем все данные.
# Конечно же вывод может быть пока что быть совсем непонятен, но мы приведем его в дальнейшем к нужному виду.
#
# Почему важно представлять себе адреса сайтов как “файлы”, а не как “страницы”? Большинство современных веб страниц связано с множеством файлов ресурсов.
# Ими могут быть файлы изображений, скриптов JavaScript, стилей CSS и любой другой контент, на который ссылается запрашиваемая страница. Например, встретив тег
# браузер знает: чтобы сгенерировать страницу для пользователя, нужно сделать еще один запрос к серверу и получить данные из файла 1.jpg.
# Разумеется, у нашего скрипта на Python нет логики, позволяющей вернуться и запросить несколько файлов (пока что), он читает только тот HTML-файл,
# который мы запросили напрямую:
#
# from urllib.request import urlopen
# Эта строка делает именно то, что кажется на первый взгляд: находит модуль Python для запросов (в библиотеке urllib) и импортирует оттуда одну функцию — urlopen.
# Библиотека urllib — это стандартная библиотека Python (другими словами, для запуска данного примера ничего не нужно устанавливать дополнительно),
# в которой содержатся функции для запроса данных через Интернет, обработки файлов cookie и даже изменения метаданных,
# таких как заголовки и пользовательский программный агент. Мы будем активно применять urllib, а также ее аналог - стороннюю библиотеку request.
# Функция urlopen открывает удаленный объект по сети и читает его. Поскольку это практически универсальная функция (она одинаково легко читает HTML-файлы,
# файлы изображений и другие файловые потоки), мы будем довольно часто использовать ее.
#
# Давайте пока познакомимся немного с самим Beautiful Soup.
# Библиотека BeautifulSoup названа так в честь одноименного стихотворения Льюиса Кэрролла из книги «Алиса в Стране чудес».
# Приложение BeautifulSoup помогает отформатировать и упорядочить «грязные» сетевые данные, исправляя ошибки в HTML-коде и создавая легко обходимые объекты Python,
# являющиеся представлениями структур XML.
#
# Поскольку BeautifulSoup не является стандартной библиотекой Python, ее необходимо установить
#  pip install beautifulsoup4
# Из всех объектов библиотеки BeautifulSoup чаще всего используется собственно, сам BeautifulSoup. Посмотрим, как он работает, изменив пример,написанный ранее

def a2():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup

    url = 'https://ru.wikipedia.org/wiki/Python'

    html = urlopen(url)
    bs = BeautifulSoup(html.read(), 'html.parser')
    print(bs)



# Отлично! Как мы видим результат в консоли уже более читаем. А теперь давайте попробуем просто вывести заголовок этой страницы

def a3():
    from urllib.request import urlopen
    from bs4 import BeautifulSoup

    url = 'https://ru.wikipedia.org/wiki/Python'

    html = urlopen(url)
    bs = BeautifulSoup(html.read(), 'html.parser')
    print(bs.h1) # про что эта страница (>Python<) - ЧИТАЕТ 1 ЗАГОЛОВОК


# Результат выглядит так:
# <h1 class="firstHeading mw-first-heading" id="firstHeading"><span class="mw-page-title-main">Python</span></h1>
#
# Обратите внимание: этот код возвращает только первый попавшийся ему на странице экземпляр тега h1.
# По существующему соглашению на странице может быть только один тег h1, однако принятые в Интернете соглашения часто нарушаются.
# Поэтому следует помнить, что таким образом будет получен только первый экземпляр тега и не обязательно тот, который вы ищете.
#
# Разберем код немного подробнее.
# Мы импортируем функцию urlopen и вызываем html.read(), чтобы получить контент страницы в формате HTML.
# Помимо текстовой строки, BeautifulSoup также может принимать файловый объект, непосредственно возвращаемый функцией urlopen.
# Чтобы получить этот объект, не нужно вызывать функцию .read():
#  bs = BeautifulSoup(html, 'html.parser')
# Здесь контент HTML-файла преобразуется в объект BeautifulSoup, имеющий следующую структуру:
# html ->  <html><head>...</head><body>...</body></html>
# head -> <head><title>Python — Википедия</title></head>
# title ->  <title>Python — Википедия</title>
# body -> <body><h1>...</h1><div>...</div></body>
# h1 ->     <h1 class="firstHeading mw-first-heading" id="firstHeading"><span class="mw-page-title-main">Python</span></h1>
# div ->    <div>....</div>
#
# Обратите внимание: тег h1, извлеченный из кода страницы, находится на втором уровне структуры объекта BeautifulSoup (html.body.h1).
# Однако, извлекая h1 из объекта, мы обращаемся к этому тегу напрямую:
# bs.h1
# На практике все следующие вызовы функций приведут к одинаковым результатам: bs.html.body.h1
# bs.body.h1
# bs.html.h1
#
# При создании объекта BeautifulSoup функции передаются два аргумента: bs=BeautifulSoup(html.read(), 'html.parser')
# Первый аргумент — это текст в формате HTML, на основе которого строится объект, а второй — синтаксический анализатор,
# который BeautifulSoup будет использовать для построения объекта.
# В большинстве случаев не имеет значения, какой именно синтаксический анализатор будет применяться. Анализатор html.parser входит в состав Python 3 и не требует дополнительной настройки перед использованием. За редким исключением, мы будем применять именно его.
# Еще один популярный анализатор — lxml (http://lxml.de/parsing.html). Он устанавливается через pip:
# pip3 install lxml
# Для того чтобы использовать lxml в BeautifulSoup, нужно изменить имя синтаксического анализатора в знакомой нам строке:
# bs = BeautifulSoup(html.read(), 'lxml')
#
# Преимущество lxml, по сравнению с html.parser, состоит в том, что lxml в целом лучше справляется с «грязным» или искаженным HTML-кодом.
# Анализатор lxml прощает неточности и исправляет такие проблемы, как незакрытые и неправильно вложенные теги, а также отсутствующие теги head или body.
# Кроме того, lxml работает несколько быстрее, чем html.parser, хотя при веб-скрапинге скорость анализатора не всегда является преимуществом,
# поскольку почти всегда главное узкое место — скорость самого сетевого соединения. Недостатками анализатора lxml является то,
# что его необходимо специально устанавливать и он зависит от сторонних C-библиотек. Это может вызвать проблемы портируемости, кроме того,
# html.parser проще в использовании.
#
# Еще один популярный синтаксический анализатор HTML называется html5lib. Подобно lxml,
# он чрезвычайно лоялен к ошибкам и прилагает еще больше усилий к исправлению некорректного HTML-кода.
# Он также имеет внешние зависимости и работает медленнее, чем lxml и html.parser.
# Тем не менее выбор html5lib может быть оправданным при работе с “грязными” или написанными вручную HTML страницами.
# Чтобы использовать этот анализатор, нужно установить его и передать объекту BeautifulSoup строку html5lib:
# bs = BeautifulSoup(html.read(), 'html5lib')
#
# Думаю уже понятно представление о возможностях и удобстве данной библиотеки.
# В сущности, она позволяет извлечь любую информацию из любого файла в формате HTML (или XML),
# если содержимое данного файла заключено в идентифицирующий тег или этот тег хотя бы присутствует в принципе.
# Позже мы подробно рассмотрим более сложные вызовы функций библиотеки,
# а также регулярные выражения и способы их использования с помощью BeautifulSoup для извлечения информации с сайтов.
#
# Теперь попробуем сделать тоже самое с помощью библиотеки requests.
# pip install requests

def a4():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://ru.wikipedia.org/wiki/Python'

    html = requests.get(url)
    bs = BeautifulSoup(html.text, 'html.parser')
    print(bs.body.h1)

# Результат будем прежним. Разберем как работает данный код:
# Первые три строки думаю разбора не требуют.
#
# html = requests.get(url)
# В данной строке мы используем метод get для запроса на получение данных по нужному адресу. Метод get используется при запросах, которые не должны изменять информацию на сервере, например мы хотим, как в данном случае получить страничку.  Существует еще и метод post, который уже указывает на то, что мы хотим, наоборот отправить какие либо данные. Давайте немного модифицируем наш код и сделаем так, чтобы результат не выводился в консоль, а сохранился в виде html файла на локальном компьютере.
def a5():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://ru.wikipedia.org/wiki/Python'

    html = requests.get(url)
    bs = BeautifulSoup(html.text, 'html.parser')
    with open('python.html', 'w', encoding='utf-8') as file:
       file.write(str(bs))

#
# В примере выше для сохранения страницы, мы открываем файл python.html на запись, если файла не существует,
# то как мы помним он создается. Также так как файл может быть не в нужной нам кодировке кодируем его в кодировку utf-8,
# на у дальше преобразовываем объект beautiful soup к строчному виду при сохранении.


#*РЕШЕНИЕ ЗАДАЧ*
# Задача 1
# Написать код, который будет выводить в консоль заголовок и единственный div странички https://www.pythonscraping.com/pages/page1.html
#Решение

def a6():
    import requests

    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page1.html'

    html = requests.get(url)
    bs = BeautifulSoup(html.text, 'html.parser')
   # print(bs.title)
    print(bs.body.div)

# Данные в  сети достаточно часто плохо отформатированы, сайты то и дело ‘падают’, а разработчики страниц часто забывают ставить закрывающие теги. Один из самых неприятных моментов, связанных с веб-скрапингом, уйти спать и оставить работающий скрапер, рассчитывая назавтра иметь все данные, а утром обнаружить, что скрапер столкнулся с ошибкой в каком-то непредсказуемом формате данных и почти сразу прекратил работу, стоило вам отвернуться от экрана. Рассмотрим первую строку нашего скрапера, сразу после операторов импорта, и подумаем, как можно обрабатывать любые исключения, которые могли бы здесь возникнуть:
# html = urlopen('http://www.pythonscraping.com/pages/page1.html')
#
# Здесь могут случиться две основные неприятности:
# • на сервере нет такой страницы (или при ее получении произошла ошибка);
# • нет такого сервера.
# В первой ситуации будет возвращена ошибка HTTP. Это может быть 404 Page Not Found, 500 Internal Server Error и т.п. Во всех таких случаях функция urlopen выдаст обобщенное исключение HTTPError. Его можно обработать следующим образом:
#

def a7():
    from bs4 import BeautifulSoup
    from urllib.request import urlopen
    from urllib.error import HTTPError
    try:
       html = urlopen('http://www.pythonscraping.com/pages/page1.html')
    except HTTPError as e: # перехвать ошибки
       print(e)
    else:
       bs = BeautifulSoup(html.read(), 'html.parser')
       print(bs.h1)

# Теперь в случае возвращения кода HTTP-ошибки выводится сообщение о ней и остальная часть программы, которая находится в ветви else, не выполняется.
# Если не найден весь сервер, то функция urlopen возвращает URLError. Эта ошибка говорит о том, что ни один из указанных серверов не доступен. Поскольку именно удаленный сервер отвечает за возвращение кодов состояния HTTP, ошибка HTTPError не может быть выдана и вместо нее следует обрабатывать более серьезную ошибку URLError. Для этого можно добавить в программу такую проверку:
#

def a8():
    from urllib.request import urlopen
    from urllib.error import HTTPError
    from urllib.error import URLError
    try:
       html = urlopen('https://pythonscrapingthisurldoesnotexist.com')
    except HTTPError as e:
       print(e)
    except URLError as e:
       print('Сервер не найден!')
    else:
       print('Работает!')


# Конечно, даже если страница успешно получена с сервера, все равно остается проблема с ее контентом, который не всегда соответствует ожидаемому. Всякий раз, обращаясь к тегу в объекте BeautifulSoup, разумно добавить проверку того, существует ли этот тег. При попытке доступа к несуществующему тегу BeautifulSoup возвращает объект None. Проблема в том, что попытка обратиться к тегу самого объекта None приводит к возникновению ошибки AttributeError.
# Следующая строка (в которой nonExistentTag — несуществующий тег, а не имя реальной функции BeautifulSoup) возвращает объект None:
# print(bs.nonExistentTag)
#
# Этот объект вполне доступен для обработки и проверки. Проблема возникает в том случае, если продолжать его использовать без проверки и попытаться вызвать для объекта None другую функцию:
# print(bs.nonExistentTag.someTag)
# Эта функция вернет исключение:
# AttributeError: 'NoneType' object has no attribute 'someTag'
#
# Как же застраховаться от этих ситуаций? Проще всего — явно проверить обе ситуации
def a9():

    try:
        badContent = bs.nonExistingTag.anotherTag
    except AttributeError as e:
        print('Тег не найден')
    else:
        if badContent == None:
            print ('Тег не найден')
        else:
            print(badContent)


# Такие проверка и обработка каждой ошибки поначалу могут показаться сложными и трудоемкими, однако если немного упорядочить код,
# то его станет проще писать (и, что еще важнее, гораздо проще читать). Вот, например, все тот же наш парсер, написанный немного по-другому:
def a10():
    from urllib.request import urlopen
    from urllib.error import HTTPError
    from bs4 import BeautifulSoup


    def getTitle(url):
       try:
           html = urlopen(url)
       except HTTPError as e:
           return None
       try:
           bs = BeautifulSoup(html.read(), 'html.parser')
           title = bs.body.h1
       except AttributeError as e:
           return None
       return title


    title = getTitle('http://www.pythonscraping.com/pages/page1.html')
    if title == None:
       print('Заголовок не был найден')
    else:
       print(title)


# В этом примере мы создаем функцию getTitle, которая возвращает либо заголовок страницы, либо, если получить его не удалось, — объект None. Внутри getTitle мы, как в предыдущем примере, проверяем наличие HTTPError и инкапсулируем две строки BeautifulSoup внутри оператора try. Ошибка AttributeError может возникнуть в любой из этих строк (если сервер не найден, то html вернет объект None, а html.read() выдаст AttributeError). Фактически внутри оператора try можно разместить любое количество строк или вообще вызвать другую функцию, которая будет генерировать AttributeError в любой момент.
#
# При написании парсеров важно продумать общий шаблон кода, который бы обрабатывал исключения, но при этом был бы читабельным. Наличие обобщенных функций, таких как getSiteHTML и getTitle (в сочетании с тщательной обработкой исключений), позволяет быстро — и надежно — собирать данные с веб-страниц в Сети
#
# Учитель: Со многими возможностями beautiful soup и requests мы познакомимся в дальнейшем, а теперь давайте немного поэкспериментируем на тестовой страничке
# https://www.pythonscraping.com/pages/page3.html
#
# Создадим парсер, который разберет нашу страничку и вытащит название и цену первого предмета. Создадим заготовку


def a11():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)
    bs = BeautifulSoup(r.text, 'html.parser')
    print(bs)

# Страничку мы получили. Теперь посмотрим как вытащить все необходимые данные. Открываем страничку в браузере(в примере используется chrome) и нажимаем клавишу f12
# Как мы видим открылась консоль разработчика. Во вкладке элементы мы видим структуру нашей странички.  Если посмотреть подробнее, то мы увидим, что нужные нам данные хранятся в таблице. О подробной структуре html документа мы поговорим позднее.
# Как мы помним, через объект bs мы можем напрямую обращаться к тегам в нашем документе. Давайте вытащим таблицу и посмотрим, что в ней
def a12():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)
    bs = BeautifulSoup(r.text, 'html.parser')
    #print(bs)

    bs = BeautifulSoup(r.text, 'html.parser')
    print(bs.table)

# Так, замечательно. Таблицу мы достали. Теперь же нам каким то образом, выдернуть необходимые данные из нее.
# Если мы укажем, что необходимо вывести тег tr

def a13():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)
    bs = BeautifulSoup(r.text, 'html.parser')
    # print(bs)
    bs = BeautifulSoup(r.text, 'html.parser')

    print(bs.table.tr)#берем данные из таблицы

# то мы получим первую строку таблицы с заголовками. Данный вариант нам не подходит.
# Доберемся до необходимого для нас элемента с помощью метода find(), который принимает имя тега и его  класс или id.

def a14():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)



    bs = BeautifulSoup(r.text, 'html.parser')
    table = bs.table
    tr = table.find('tr', 'gift')
    print(tr)



# В данном случае мы получили первый элемент. Теперь требуется вытащить уже из него данные.
# Нам требуется получить все дочерний элементы tr. Для этого воспользуемся списком дочерних элементов contents

def a15():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)



    bs = BeautifulSoup(r.text, 'html.parser')
    table = bs.table
    tr = table.find('tr', 'gift')
    print(tr)
    # Отлично теперь мы можем вывести необходимые значения
    print(tr.contents[0])
    print(tr.contents[2])




def a16():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)

    bs = BeautifulSoup(r.text, 'html.parser')
    table = bs.table
    tr = table.find('tr', 'gift')
    print(tr)
    # Отлично теперь мы можем вывести необходимые значения
    print(tr.contents[0].text)
    print(tr.contents[2].text)




# Значения мы получили, но не мешало бы избавится от тегов. Для получения только текста мы можем напрямую указать text



# Уже близко, осталось только убрать лишние пробелы. Так как мы получаем строку, то можем воспользоваться методом строк strip(), который удаляет пробелы.

def a17():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://www.pythonscraping.com/pages/page3.html'

    r = requests.get(url)

    bs = BeautifulSoup(r.text, 'html.parser')
    table = bs.table
    tr = table.find('tr', 'gift')
    print(tr)
    # Отлично теперь мы можем вывести необходимые значения
    print(tr.contents[0].text)
    print(tr.contents[2].text)
    title = tr.contents[0].text.strip()
    price = tr.contents[2].text.strip()
    print(title, price, sep=':')

# Данный вариант не самый оптимальный, но все же позволяет нам выполнить какие то действия.
# В дальнейшем мы более подробно познакомимся с библиотекой beautiful soup, но прежде на следующих занятиях мы немного углубимся в структуру html документа.



#
# 2. *РЕШЕНИЕ ЗАДАЧ*
# Задача 1
# Написать код, который выводит текст из тега h1 на странице википедии в формате Заголовок: …
def a18():
    import requests
    from bs4 import BeautifulSoup

    url = 'https://ru.wikipedia.org/wiki/%D0%AF%D0%B7%D1%8B%D0%BA_%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F'

    r = requests.get(url)
    bs = BeautifulSoup(r.text, 'html.parser')
    result = bs.h1.text
    print(f'Заголовок: {result}')
a18()

#Домашняя работа - Задача 1 - Спарсить заголовок главной страницы Википедии
a3()